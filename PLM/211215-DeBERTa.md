## **DeBERTa ë…¼ë¬¸ ë¦¬ë·°** (12ì›” 15ì¼~ğŸƒâ€â™€ï¸)

### [ğŸ“„**Paper**](https://openreview.net/pdf?id=XPZIaotutsD)  
P. He, X. Liu, J. Gao, W. Chen, "Deberta: decoding-enhanced bert with disentangled attention," _Proc. of the 9th International Conference on Learning Representations (ICLR 2021)_, Online, 2021.

### **ğŸ“Œ ëª©ì°¨** 

1. Introduction
2. Background   
    2.1 Transformer   
    2.2 Masked language model    
3. The DeBERTa architecture  
    3.1 Disentangled attention: A two-vector approach to content and position embedding   
    3.2 Enhanced mask decoder accounts for absolute word positions    
4. Scale invariant fine-tuning   
5. Experiment
    5.1 Main results on NLU tasks     
    5.2 Model analysis  
    5.3 Scale up to 1.5 billion parameters  
6. Conclusions  

---

### **1. Introduction**

íŠ¸ëœìŠ¤í¬ë¨¸ì— ê¸°ë°˜í•œ Pretrained Language Model(PLM)ì´ ë§ì€ NLP taskì—ì„œ SOTAë¥¼ ë‹¬ì„±í•˜ê³  ìˆë‹¤.  

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” *Disentangled Attention mechanism*ê³¼ *Enhanced Mask Decoder*ë¥¼ í†µí•´ ìƒˆë¡œìš´ SOTAë¥¼ ë‹¬ì„±í•œ DeBERTa ëª¨ë¸ì„ ì œì•ˆí•˜ì˜€ë‹¤.      

- **âœ¨ Disentangled Attention mechanism**  
ë‹¨ì–´ ê°„ attention weightëŠ” ë‚´ìš© ë¿ë§Œ ì•„ë‹ˆë¼ ìœ„ì¹˜ ì •ë³´ë„ ìƒë‹¹í•œ ì˜í–¥ì„ ë¼ì¹˜ê¸° ë•Œë¬¸ì—   
Contentì™€ Relative Position ì •ë³´ë¥¼ 2ê°œì˜ ë²¡í„°ë¡œ ë¶„ë¦¬í•˜ì—¬ Cross Attention ìˆ˜í–‰  

- **âœ¨ Enhanced Mask Decoder**  
ë¬¸ë²•ì  ë‰˜ì•™ìŠ¤ëŠ” ë‹¨ì–´ì˜ ìƒëŒ€ì  ìœ„ì¹˜ ì •ë³´ê°€ ì•„ë‹Œ ì ˆëŒ€ì  ìœ„ì¹˜ ì •ë³´ì— ì˜í–¥ì„ ë°›ëŠ”ë‹¤.  
ë¬¸ë²•ì  ë‰˜ì•™ìŠ¤ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ ë””ì½”ë”© ì‹œ, absolute position embeddingì„ ì·¨í•©í•˜ì—¬ ì‚¬ìš©í•œë‹¤.    

ë˜í•œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” perturbationì— ëŒ€í•œ robustnessë¥¼ ê°œì„ í•˜ë©´ì„œ ì•ˆì •ëœ ì„±ëŠ¥ì„ ë³´ì´ëŠ” *SiFT algorithm*ë„ í•¨ê»˜ ì œì•ˆí•˜ì˜€ë‹¤.  

<div align=center>
<img src="../img/DeBERTa/position.jpeg" width=500/><br>
Relative position vs Absolute position</div>
<br>

#### **â˜€ï¸ ê°œì„ í•œ ë‚´ìš©**
1. ì‚¬ì „í•™ìŠµ íš¨ìœ¨  
ì‚¬ì „ í•™ìŠµ ë°ì´í„°ê°€ ì ì–´ë„ ì„±ëŠ¥ ìš°ìˆ˜ 

2. NLU, NLG taskì—ì„œì˜ SOTA

<br>

### **2. Background**

#### **2.1 Transformer**  
ê¸°ì¡´ì˜ TransformerëŠ” ì…ë ¥ sentenceê°€ matrixë¡œ ë“¤ì–´ê°€ê¸° ë•Œë¬¸ì—, ìˆœì°¨ ì •ë³´ë¥¼ í¬í•¨í•˜ì§€ ì•ŠëŠ”ë‹¤.  
ë”°ë¼ì„œ Positional Embeddingì„ í†µí•´ ìœ„ì¹˜ ì •ë³´ë¥¼ í¬í•¨ì‹œí‚¤ëŠ”ë°, 
ì´ëŸ¬í•œ ìœ„ì¹˜ ì •ë³´ëŠ” ë‹¤ìŒ ë‘ ê°€ì§€ë¡œ êµ¬ë¶„ë  ìˆ˜ ìˆë‹¤.  
- Absolute Position  
- Relative Position 

Shaw et al. (2018)ì˜ ì—°êµ¬ì— ë”°ë¥´ë©´ Relative position ì •ë³´ê°€ NLU/NLG taskì— ë” íš¨ê³¼ì ì´ë¼ê³  ë°í˜€ì¡Œë‹¤.


#### **2.2 Masked language model**

Transformerì— ê¸°ë°˜í•œ PLMì€ ëŒ€ë¶€ë¶„ MLM taskë¡œ ì‚¬ì „í•™ìŠµì„ ìˆ˜í–‰í•œë‹¤.  
ì „ì²´ ì‹œí€€ìŠ¤ì˜ 15%ë¥¼ ë§ˆìŠ¤í‚¹í•œ $\tilde{X}$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì´ë¥¼ ë³µì›í•œ $X$ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµí•œë‹¤

$$
\max_\theta \log{p_\theta(X|\tilde{X})} = \max_\theta \sum_{i\in \mathcal{C}} \log{p_\theta (\tilde{x_i}= x_i|\tilde{X})}
$$
where $\mathcal{C}$ëŠ” masking ëœ ë‹¨ì–´ì˜ index ë¦¬ìŠ¤íŠ¸ 


### **3. The DeBERTa architecture**  

#### **3.1 Disentangled attention: A two-vector approach to content and position embedding**

<br>

$\{H_i\}$ : ië²ˆì©¨ tokenì— ëŒ€í•œ content vector   
$\{P_{i|j}\}$ : ië²ˆì§¸ tokenì—ì„œ jë²ˆì§¸ tokenì— ëŒ€í•œ relative position vector 


contentì™€ positionì— ëŒ€í•œ ë²¡í„°ë¥¼ ë¶„ë¦¬í–ˆì„ ë•Œ Cross attention score ê³„ì‚°ì€ ì•„ë˜ì™€ ê°™ì´ 4ê°œì˜ ì»´í¬ë„ŒíŠ¸ì— ëŒ€í•œ ë§ì…ˆìœ¼ë¡œ ë¶„ë¦¬ë  ìˆ˜ ìˆë‹¤.

$$
A_{i,j} = \{H_i, P_{i|j}\} \times  \{H_j, P_{j|i}\}^\intercal
$$
$$
= H_iH^\intercal_j + H_iP^\intercal_{j|i} + P_{i|j}H^\intercal_j + P_{i|j}P^\intercal_{j|i}
$$

ê°ê°ì„ *content-to-content*, *content-to-position*, *position-to-content*, *position-to-position* ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤

> ğŸ’¡ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ìƒëŒ€ì ì¸ ìœ„ì¹˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—, position-to-position termì€ í•„ìš” ì—†ì–´ ì œì™¸í•˜ì—¬ ê³„ì‚°í•˜ì˜€ë‹¤.


ì•„ë˜ëŠ” relative position ì •ë³´ë¥¼ ì„ë² ë”©í•˜ì—¬ attentionì„ ì ìš©í•œ ê¸°ì¡´ Shaw et al. (2018) ì—°êµ¬ì—ì„œì˜ Attention score ê³„ì‚° ì‹ìœ¼ë¡œ,   
*content-to-content*, *content-to-position*ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. 

<div align=center>
<img src="../img/DeBERTa/shaw_attn_score.jpeg" width=500/></div>

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” relative position ì •ë³´ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•´ $\delta$ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í–ˆëŠ”ë°,  
$N$ê°œì˜ ì‹œí€€ìŠ¤ì— ëŒ€í•´ ëª¨ë“  ìƒëŒ€ì  ìœ„ì¹˜ ì •ë³´ë¥¼ ì €ì¥í•˜ì§€ ì•Šê³  ìƒí•œì„ ì„ ë‘ì–´ $k$ ê±°ë¦¬ê¹Œì§€ë§Œ ì €ì¥í•œë‹¤ 

<div align=center>
<img src="../img/DeBERTa/eq_3.jpeg" width=500/></div>
<br>

> ğŸ’¡ **ê³µê°„ ë³µì¡ë„(Space Complexity) ê°ì†Œ**    
> 
> ìœ„ì™€ ê°™ì´ $\delta$ í•¨ìˆ˜ë¥¼ í†µí•´ ëª¨ë“  relative positionì„ 0~$2k$ë¡œ ë§¤í•‘í–ˆê¸° ë•Œë¬¸ì— $Q, K$ì„ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°,  
Nê°œì˜ query ì‹œí€€ìŠ¤ì— ëŒ€í•œ relative position embeddingì„ ìƒˆë¡œ í• ë‹¹í•˜ì§€ ì•Šì•„ë„ ë˜ê¸° ë•Œë¬¸ì— space complexityê°€ $O(N^2d)$ì—ì„œ $O(kd)$ë¡œ ê°ì†Œí•˜ì˜€ë‹¤.
> 

<br>

<div align=center>
<img src="../img/DeBERTa/eq_4.jpeg" width=900/><br>
Attention Output ê³„ì‚° ê³¼ì •</div>
<br>

> ğŸ’¡ **$\delta(i,j)$ê°€ ì•„ë‹Œ $\delta(j,i)$ì¸ ì´ìœ **  
>
> í•´ë‹¹ termì€ position-to-content termì´ë©°,   
> ì£¼ì–´ì§„ query position iì— ëŒ€í•œ jë²ˆì§¸ tokenì˜ key contentì˜ attention weightë¥¼ ì—°ì‚°í•´ì•¼ í•œë‹¤    
> ***content-j, position-i***

Attention score ì—°ì‚°ì´ ëë‚˜ë©´ scaling ìˆ˜í–‰    
â†’ large scaleì˜ PLMì„ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë‹¤

<br>

#### **3.2 Enhanced mask decoder accounts for absolute word positions**

ìœ„ì˜ Disentangled Attention mechanismì€ absolute positionì„ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤.  
í•˜ì§€ë§Œ ì ˆëŒ€ ìœ„ì¹˜ ì •ë³´ëŠ” ë¬¸ë²•ì  ì¸¡ë©´ì—ì„œ í•„ìš”í•œ ìš”ì†Œì´ê¸° ë•Œë¬¸ì—, EMD (Enhanced Mask Decoder)ì—ì„œ Absolute position embeddingì„ ì·¨í•©í•œë‹¤. 




### **4. Scale invariant fine-tuning**

### **5. Experiment**

#### **â‘´ Neural Abstractive Summarization**   

#### **â‘µ Guidance**   


### **6. Conclusions**

----

### **Appendix**