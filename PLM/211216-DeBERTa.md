## **DeBERTa ë…¼ë¬¸ ë¦¬ë·°** (12ì›” 15ì¼~ğŸƒâ€â™€ï¸)

### [ğŸ“„**Paper**](https://openreview.net/pdf?id=XPZIaotutsD)  
P. He, X. Liu, J. Gao, W. Chen, "Deberta: decoding-enhanced bert with disentangled attention," _Proc. of the 9th International Conference on Learning Representations (ICLR 2021)_, Online, 2021.

### **ğŸ“Œ ëª©ì°¨** 

1. Introduction
2. Background   
    2.1 Transformer   
    2.2 Masked language model    
3. The DeBERTa architecture  
    3.1 Disentangled attention: A two-vector approach to content and position embedding   
    3.2 Enhanced mask decoder accounts for absolute word positions    
4. Scale invariant fine-tuning   
5. Experiment
    5.1 Main results on NLU tasks     
    5.2 Model analysis  
    5.3 Scale up to 1.5 billion parameters  
6. Conclusions  

---

### **1. Introduction**

íŠ¸ëœìŠ¤í¬ë¨¸ì— ê¸°ë°˜í•œ Pretrained Language Model(PLM)ì´ ë§ì€ NLP taskì—ì„œ SOTAë¥¼ ë‹¬ì„±í•˜ê³  ìˆë‹¤.  

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” *Disentangled Attention mechanism*ê³¼ *Enhanced Mask Decoder*ë¥¼ í†µí•´ ìƒˆë¡œìš´ SOTAë¥¼ ë‹¬ì„±í•œ DeBERTa ëª¨ë¸ì„ ì œì•ˆí•˜ì˜€ë‹¤.      

- **âœ¨ Disentangled Attention mechanism**  
ë‹¨ì–´ ê°„ attention weightëŠ” ë‚´ìš© ë¿ë§Œ ì•„ë‹ˆë¼ ìœ„ì¹˜ ì •ë³´ë„ ìƒë‹¹í•œ ì˜í–¥ì„ ë¼ì¹˜ê¸° ë•Œë¬¸ì—   
Contentì™€ Relative Position ì •ë³´ë¥¼ 2ê°œì˜ ë²¡í„°ë¡œ ë¶„ë¦¬í•˜ì—¬ Cross Attention ìˆ˜í–‰  

- **âœ¨ Enhanced Mask Decoder**  
ë¬¸ë²•ì  ë‰˜ì•™ìŠ¤ëŠ” ë‹¨ì–´ì˜ ìƒëŒ€ì  ìœ„ì¹˜ ì •ë³´ê°€ ì•„ë‹Œ ì ˆëŒ€ì  ìœ„ì¹˜ ì •ë³´ì— ì˜í–¥ì„ ë°›ëŠ”ë‹¤.  
ë¬¸ë²•ì  ë‰˜ì•™ìŠ¤ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ ë””ì½”ë”© ì‹œ, absolute position embeddingì„ ì·¨í•©í•˜ì—¬ ì‚¬ìš©í•œë‹¤.    

ë˜í•œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” perturbationì— ëŒ€í•œ robustnessë¥¼ ê°œì„ í•˜ë©´ì„œ ì•ˆì •ëœ ì„±ëŠ¥ì„ ë³´ì´ëŠ” *SiFT algorithm*ë„ í•¨ê»˜ ì œì•ˆí•˜ì˜€ë‹¤.  

<div align=center>
<img src="../img/DeBERTa/position.jpeg" width=500/><br>
Relative position vs Absolute position</div>
<br>

#### **â˜€ï¸ ê°œì„ í•œ ë‚´ìš©**
1. ì‚¬ì „í•™ìŠµ íš¨ìœ¨  
ì‚¬ì „ í•™ìŠµ ë°ì´í„°ê°€ ì ì–´ë„ ì„±ëŠ¥ ìš°ìˆ˜ 

2. NLU, NLG taskì—ì„œì˜ SOTA

<br>

### **2. Background**

#### **2.1 Transformer**  
ê¸°ì¡´ì˜ TransformerëŠ” ì…ë ¥ sentenceê°€ matrixë¡œ ë“¤ì–´ê°€ê¸° ë•Œë¬¸ì—, ìˆœì°¨ ì •ë³´ë¥¼ í¬í•¨í•˜ì§€ ì•ŠëŠ”ë‹¤.  
ë”°ë¼ì„œ Positional Embeddingì„ í†µí•´ ìœ„ì¹˜ ì •ë³´ë¥¼ í¬í•¨ì‹œí‚¤ëŠ”ë°, 
ì´ëŸ¬í•œ ìœ„ì¹˜ ì •ë³´ëŠ” ë‹¤ìŒ ë‘ ê°€ì§€ë¡œ êµ¬ë¶„ë  ìˆ˜ ìˆë‹¤.  
- Absolute Position  
- Relative Position 

Shaw et al. (2018)ì˜ ì—°êµ¬ì— ë”°ë¥´ë©´ Relative position ì •ë³´ê°€ NLU/NLG taskì— ë” íš¨ê³¼ì ì´ë¼ê³  ë°í˜€ì¡Œë‹¤.


#### **2.2 Masked language model**

Transformerì— ê¸°ë°˜í•œ PLMì€ ëŒ€ë¶€ë¶„ MLM taskë¡œ ì‚¬ì „í•™ìŠµì„ ìˆ˜í–‰í•œë‹¤.  
ì „ì²´ ì‹œí€€ìŠ¤ì˜ 15%ë¥¼ ë§ˆìŠ¤í‚¹í•œ $\tilde{X}$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì´ë¥¼ ë³µì›í•œ $X$ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµí•œë‹¤

$$
\max_\theta \log{p_\theta(X|\tilde{X})} = \max_\theta \sum_{i\in \mathcal{C}} \log{p_\theta (\tilde{x_i}= x_i|\tilde{X})}
$$
where $\mathcal{C}$ëŠ” masking ëœ ë‹¨ì–´ì˜ index ë¦¬ìŠ¤íŠ¸ 


### **3. The DeBERTa architecture**  

#### **3.1 Disentangled attention: A two-vector approach to content and position embedding**

<br>

$\{H_i\}$ : ië²ˆì©¨ tokenì— ëŒ€í•œ content vector   
$\{P_{i|j}\}$ : ië²ˆì§¸ tokenì—ì„œ jë²ˆì§¸ tokenì— ëŒ€í•œ relative position vector 


contentì™€ positionì— ëŒ€í•œ ë²¡í„°ë¥¼ ë¶„ë¦¬í–ˆì„ ë•Œ Cross attention score ê³„ì‚°ì€ ì•„ë˜ì™€ ê°™ì´ 4ê°œì˜ ì»´í¬ë„ŒíŠ¸ì— ëŒ€í•œ ë§ì…ˆìœ¼ë¡œ ë¶„ë¦¬ë  ìˆ˜ ìˆë‹¤.

$$
A_{i,j} = \{H_i, P_{i|j}\} \times  \{H_j, P_{j|i}\}^\intercal
$$
$$
= H_iH^\intercal_j + H_iP^\intercal_{j|i} + P_{i|j}H^\intercal_j + P_{i|j}P^\intercal_{j|i}
$$

ê°ê°ì„ *content-to-content*, *content-to-position*, *position-to-content*, *position-to-position* ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤

> ğŸ’¡ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ìƒëŒ€ì ì¸ ìœ„ì¹˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—, position-to-position termì€ í•„ìš” ì—†ì–´ ì œì™¸í•˜ì—¬ ê³„ì‚°í•˜ì˜€ë‹¤.


ì•„ë˜ëŠ” relative position ì •ë³´ë¥¼ ì„ë² ë”©í•˜ì—¬ attentionì„ ì ìš©í•œ ê¸°ì¡´ Shaw et al. (2018) ì—°êµ¬ì—ì„œì˜ Attention score ê³„ì‚° ì‹ìœ¼ë¡œ,   
*content-to-content*, *content-to-position*ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. 

<div align=center>
<img src="../img/DeBERTa/shaw_attn_score.jpeg" width=500/></div>

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” relative position ì •ë³´ë¥¼ ë§¤í•‘í•˜ê¸° ìœ„í•´ $\delta$ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í–ˆëŠ”ë°,  
$N$ê°œì˜ ì‹œí€€ìŠ¤ì— ëŒ€í•´ ëª¨ë“  ìƒëŒ€ì  ìœ„ì¹˜ ì •ë³´ë¥¼ ì €ì¥í•˜ì§€ ì•Šê³  ìƒí•œì„ ì„ ë‘ì–´ $k$ ê±°ë¦¬ê¹Œì§€ë§Œ ì €ì¥í•œë‹¤ 

<div align=center>
<img src="../img/DeBERTa/eq_3.jpeg" width=500/></div>
<br>

> ğŸ’¡ **ê³µê°„ ë³µì¡ë„(Space Complexity) ê°ì†Œ**    
> 
> ìœ„ì™€ ê°™ì´ $\delta$ í•¨ìˆ˜ë¥¼ í†µí•´ ëª¨ë“  relative positionì„ 0~$2k$ë¡œ ë§¤í•‘í–ˆê¸° ë•Œë¬¸ì— $Q, K$ì„ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°,  
> 
>Nê°œì˜ tokenì— ëŒ€í•œ relative position embeddingì„ ìƒˆë¡œ í• ë‹¹í•˜ì§€ ì•Šì•„ë„ ë˜ê¸° ë•Œë¬¸ì— space complexityê°€ $O(N^2d)$ì—ì„œ $O(kd)$ë¡œ ê°ì†Œí•˜ì˜€ë‹¤.
> 

<br>

<div align=center>
<img src="../img/DeBERTa/eq_4.jpeg" width=900/><br>
Attention Output ê³„ì‚° ê³¼ì •</div>
<br>

Attention score ì—°ì‚°ì´ ëë‚˜ë©´ scaling ìˆ˜í–‰    
â†’ large scaleì˜ PLMì„ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë‹¤


> ğŸ’¡ **$\delta(i,j)$ê°€ ì•„ë‹Œ $\delta(j,i)$ì¸ ì´ìœ **  
>
> í•´ë‹¹ termì€ position-to-content termì´ë©°,   
> ì˜ˆì‹œë¥¼ ë“¤ë©´ ì•„ë˜ì™€ ê°™ë‹¤  
> $k$ê°€ 2ì¼ ë•Œ relative positionì„ êµ¬í•˜ëŠ” ì˜ˆì‹œì´ë‹¤  
> <div align=center><img src="../img/DeBERTa/pos_to_cont_example(1).png" width=500/></div>  
> query position iì—ì„œ key contentì˜ relative positionì€ -1ì— í•´ë‹¹í•œë‹¤  
>
> ë”°ë¼ì„œ ì´ë¥¼ $\delta$ í•¨ìˆ˜ì— ë§¤í•‘í•˜ë ¤ë©´ $\delta(j,i)$ê°€ ë˜ì–´ì•¼ í•œë‹¤
>
> ë˜ ë‹¤ë¥¸ ì˜ˆì‹œë¥¼ ë“¤ìë©´ $i<j$ ì´ê³ , $k$ê°€ 5ì¸ ì˜ˆì‹œì´ë‹¤
> <div align=center><img src="../img/DeBERTa/pos_to_cont_example(2).png" width=500/></div>  
>
> ë§ˆì°¬ê°€ì§€ë¡œ ì£¼ì–´ì§„ query position iì—ì„œ key content jì˜ relative positionì€ +4ì— í•´ë‹¹í•œë‹¤  
> ì´ë¥¼ $\delta$ í•¨ìˆ˜ì— ë§¤í•‘í•˜ë ¤ë©´ $\delta(j,i)$ê°€ ë˜ì–´ì•¼ í•œë‹¤


ì•„ë˜ëŠ” Disentangled Attentionì˜ ì—°ì‚° ê³¼ì •ì„ ê·¸ë¦¼ìœ¼ë¡œ ê·¸ë¦° ê²ƒì´ë‹¤  
<div align=center><img src="../img/DeBERTa/deberta_enc_layer.png" width=800/><br>Disentangled Attention</div>  

<br>

<br>

#### **3.2 Enhanced mask decoder accounts for absolute word positions**

ìœ„ì˜ Disentangled Attention mechanismì€ absolute positionì„ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤.  

í•˜ì§€ë§Œ ì ˆëŒ€ ìœ„ì¹˜ ì •ë³´ëŠ” ë¬¸ë²•ì  ì¸¡ë©´ì—ì„œ í•„ìš”í•œ ìš”ì†Œì´ê¸° ë•Œë¬¸ì—, <u>EMD (Enhanced Mask Decoder)</u>ì—ì„œ Absolute position embeddingì„ ì·¨í•©í•œë‹¤. 

**Absolute positionì˜ ì˜í–¥**  
ì•„ë˜ storeì™€ mallì˜ ì£¼ë³€ ë‹¨ì–´ë¥¼ ë³´ë©´ ë˜‘ê°™ì´ newë¼ëŠ” ë‹¨ì–´ê°€ ìˆì–´, local context (relative positionê³¼ content)ë¡œëŠ” ë‘˜ì„ êµ¬ë¶„í•˜ê¸° í˜ë“¤ë‹¤. 

ë‘˜ì´ ì„œë¡œ ë‹¤ë¥¸ ê°ì²´ë¥¼ ê°€ë¦¬í‚¨ë‹¤ëŠ” ê²ƒì€ ë¬¸ë²•ì  ë‰˜ì•™ìŠ¤ë¥¼ í†µí•´ ì•Œ ìˆ˜ ìˆëŠ”ë°,  
ì´ëŸ¬í•œ ë¬¸ë²•ì  ë‰˜ì•™ìŠ¤ëŠ” ë¬¸ì¥ ë‚´ ë‹¨ì–´ë“¤ì˜ absolute position ì •ë³´ì— ì˜ì¡´í•œë‹¤

<div align=center><img src="../img/DeBERTa/absolute_pos_example.png" width=800/><br>absolute positionì— ì˜í–¥ì„ ë°›ëŠ” ì˜ˆì‹œ</div>  


**EMD**

- BERTì˜ Absolute position ì‚¬ìš© ë°©ë²•  
BERTëŠ” ì…ë ¥ë‹¨ì— position, segment, token embeddingì„ ë”í•´ ì…ë ¥ì„ êµ¬ì„±í•¨ìœ¼ë¡œì¨, absolute position ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤.  
<br><div align=center><img src="../img/DeBERTa/bert_absolute_pos.jpeg" width=800/><br><strong>BERTì˜ Input êµ¬ì„±</strong></div>  

<br><div align=center><img src="../img/DeBERTa/bert_mlm.png" width=800/><br><strong>BERTì˜ MLM í•™ìŠµ êµ¬ì¡°ë„</strong></div>  

<br>

- DeBERTaì˜ Absolute position ì‚¬ìš© ë°©ë²•  
ì…ë ¥ë‹¨ì— position ì •ë³´ë¥¼ ë”í•œ BERTì™€ ë‹¬ë¦¬, Transformer encoder layer ì§í›„ì— absolute positionì„ ì¶”ê°€í•˜ì˜€ë‹¤.   
ì´ë•Œ, EMDëŠ” ê¸°ì¡´ BERTì˜ Transformer Encoder Layerì— Absolute position embeddingì„ ì¶”ê°€í•œ ê²ƒì´ë‹¤.  


<br><div align=center><img src="../img/DeBERTa/deberta_mlm.png" width=800/><br><strong>deBERTaì˜ MLM í•™ìŠµ êµ¬ì¡°ë„</strong></div>  
<br>

BERTëŠ” ê° ê³„ì¸µì— self-attentionì„ ìˆ˜í–‰í–ˆëŠ”ë°,  
deBERTaì˜ EMDëŠ” $I$ì™€ $H$ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ $I$ë¥¼ queryë¡œ $H$ë¥¼ keyì™€ valueë¡œ ì‚¬ìš©í•˜ë©° cross attentionì„ ìˆ˜í–‰í•œë‹¤  

EMDì˜ ì²«ë²ˆì§¸ ê³„ì¸µì€ Absolute position embeddingì„ $I$ë¡œ ì‚¬ìš©í•˜ê³ , ê·¸ ë‹¤ìŒ ê³„ì¸µë¶€í„°ëŠ” ì´ì „ EMD ê³„ì¸µì˜ outputì„ $I$ë¡œ ì‚¬ìš©í•œë‹¤.

<br><div align=center><img src="../img/DeBERTa/deberta_emd_layer.png" width=800/><br><strong>deBERTaì˜ EMD Layer</strong></div>  


### **4. Scale invariant fine-tuning**

ì´ ì„¹ì…˜ì—ì„œëŠ” ì•ˆì •ì ìœ¼ë¡œ finetuning í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ ***SiFT (Scale-invariant-Fine-Tuning)*** ë¥¼ ì œì•ˆí•œë‹¤.

#### **Virtual Adversarial Training (VAT)**  

Adversarial Trainingì´ë€, ì…ë ¥ì— perturbationì„ ì¶”ê°€í•˜ì—¬ ë°ì´í„°ì— ì ëŒ€ì  ë°©í–¥(adversarial direction)ì„ ì •ì˜í•˜ì—¬ í•™ìŠµí•˜ë©° perturbationì— ëŒ€í•œ robustnessë¥¼ ê°œì„ í•˜ëŠ” í•™ìŠµì´ë‹¤.  

Virtual Adversarial TrainingëŠ” labelì´ ì—†ëŠ” ë°ì´í„°ì— ëŒ€í•´ì„œë„ 
ê°€ìƒì˜ adversarial directionì„ ì •ì˜í•˜ì—¬ í•™ìŠµí•œ ê²ƒì´ë‹¤.  


#### **VAT in NLP**
NLPì—ì„œëŠ” perturbationì„ word embeddingì— ì¶”ê°€í•˜ëŠ”ë°,  ì´ ì„ë² ë”© ê°’ì€ ë‹¨ì–´ ë° ëª¨ë¸ë§ˆë‹¤ ê·¸ ë²”ìœ„ê°€ ë§¤ìš° ë‹¤ì–‘í•˜ë‹¤.   
ë¬¸ì œëŠ” ëª¨ë¸ì˜ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ ì„ë² ë”© ê°’ì˜ varianceê°€ ì»¤ì§€ê¸° ë•Œë¬¸ì— í•™ìŠµ ë¶ˆì•ˆì •ìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆë‹¤


#### **SiFT**  
ê·¸ëƒ¥ word embeddingì— perturbationì„ ì¶”ê°€í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, normalized word embeddingì— perturbationì„ ì¶”ê°€í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤  
ëª¨ë¸ì˜ í¬ê¸°ê°€ í´ìˆ˜ë¡ ì„±ëŠ¥ ê°œì„ ì„ ëšœë ·í•˜ê²Œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.  

### **5. Experiment**

#### **5.1 Main resultss on NLU tasks** 

â‘´ Large Model

â‘µ Base Model

#### **5.2 Model analysis** 

#### **5.3 Scale up to 1.5 billion parameters** 


### **6. Conclusions**

----

### **Appendix**