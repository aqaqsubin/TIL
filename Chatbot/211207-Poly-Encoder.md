## **Facebookì˜ Poly-Encoder ë…¼ë¬¸ ë¦¬ë·°** (12ì›” 7ì¼~ğŸƒâ€â™€ï¸)

### [ğŸ“„**Paper**](https://openreview.net/pdf?id=SkxgnnNFvH)  
Humeau, S., Shuster, K., Lachaux, M. A., and Weston, J., â€œPoly-encoders: architectures and pre-training strategies for fast and accurate multi-sentence scoring,â€ _Proc. of the 8th International Conference on Learning Representations (ICLR 2020)_, Addis Ababa, Ethiopia, 2020.

### **ğŸ“Œ ëª©ì°¨** 

1. Introduction
2. Related work  
3. Tasks
4. Methods  
    4.1 Transformers and Pre-training Strategies  
    4.2 Bi-encoder  
    4.3 Cross-encoder  
    4.4 Poly-encoder
5. Experiments  
    5.1 Bi-encoders and Cross-encoders     
    5.2 Poly-encoders  
    5.3 Domain-specific pre-training  
    5.4 Inference speed  
6. Conclusion  

---

### **1. Introduction**

ê¸°ì¡´ì˜ pairwise comparision taskë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ Cross-encoder, Bi-encoder ì ‘ê·¼ë°©ì‹ì˜ ì„±ëŠ¥ ê°œì„ í•œ new Transformer architecture _Poly-encoder_ ì œì•ˆ  

**Poly-enocder**
- Cross-encoderì˜ ì¶”ë¡  ì‹œê°„ ê°œì„ 
- Bi-encoderì˜ prediction quality ê°œì„ 

**4ê°€ì§€ ë°ì´í„°ì…‹ì—ì„œì˜ Poly-encoderì„±ëŠ¥ ê²€ì¦**  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dialogue ë° Information Retrieval (IR) ë„ë©”ì¸ì— ëŒ€í•œ ë°ì´í„°ì…‹ì—ì„œ ì„±ëŠ¥ ê²€ì¦  

### **2. Related work**

1. Bi-encoder  
Inputê³¼ candidate labelì„ ê°™ì€ ê³µê°„ ìƒì— ë§¤í•‘í•œ í›„ ìœ ì‚¬ë„ë¥¼ ë¹„êµí•¨ìœ¼ë¡œì¨ scoring  
vector space model, LSI, supervised embeddings, classical siamese networks ë“±ì´ ìˆìŒ  
<br>
**next utterance prediction**ì—ë„ Memory Networks, Transformer Memory networks, LSTMs, CNNsì™€ ê°™ì´   
inputê³¼ candidate labelì„ ë”°ë¡œ ì¸ì½”ë”©í•˜ëŠ” Bi-encoder approachê°€ ì‚¬ìš©ë˜ì—ˆë‹¤.   
<br>
inputê³¼ candidatesë¥¼ ê°ê° ë”°ë¡œ ì„ë² ë”©í•˜ê¸° ë•Œë¬¸ì—, candidatesë¥¼ ì €ì¥í•˜ì—¬ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤.  
â†’ candidateë¥¼ ë‹¤ì‹œ ì„ë² ë”©í•  í•„ìš”ê°€ ì—†ê¸° ë•Œë¬¸ì— ì¶”ë¡  ì‹œê°„ì´ ë¹¨ë¼ì§

2. Cross-encoder  
Inputê³¼ candidateë¥¼ ì´ì–´ë¶™ì—¬(concatenation) í•˜ë‚˜ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ë©°, non-linear functionì— ì˜í•´ scoring ëœë‹¤.  
Sequential Matching Network CNN-based architecture, Deep Matching Networks, Gated Self-Attention ë“±ì´ ìˆë‹¤.    
<br>
ê°€ì¥ ìµœì‹  transformerë¥¼ ì´ìš©í•œ Cross-encoder approachëŠ” ê° layerì— self-attentionì„ ì ìš©í•œ ê²°ê³¼ë¥¼ ë‚´ë©°,   
inputê³¼ candidate ê°„ì˜ interactionì„ ê·¹ëŒ€í™”í•œë‹¤. (candidate ë‚´ wordëŠ” input ë‚´ ëª¨ë“  wordì— attend í•  ìˆ˜ ìˆìŒ)  

> Urbanek et al. (2019)ì˜ ì—°êµ¬ì—ì„œëŠ” ì‚¬ì „í•™ìŠµëœ BERTë¥¼ í†µí•œ Cross-encoderì™€ Bi-encoderì˜ ì„±ëŠ¥ì„ ë¹„êµí–ˆìœ¼ë©°,  
Cross-encoderì˜ ì ‘ê·¼ ë°©ì‹ì´ ì„±ëŠ¥ì´ ë” ë›°ì–´ë‚˜ì§€ë§Œ ì¶”ë¡  ì‹œê°„ ë©´ì—ì„œëŠ” ë’¤ì³ì¡Œë‹¤.

<br>

### **3. Tasks**

ë‹¤ìŒ ë‘ ê°€ì§€ ë„ë©”ì¸ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì…‹ì„ í†µí•´ ë³¸ ì—°êµ¬ì—ì„œ ì œì•ˆí•˜ëŠ” Poly-encoder ì„±ëŠ¥ì„ ê²€ì¦í•˜ì˜€ë‹¤.  

**Sentence Selection in Dialogue**

- ConvAI2 dataset  
ëŒ€í™” ë°ì´í„°ë¡œ, ê° ë°œí™”ìì— ëŒ€í•œ í˜ë¥´ì†Œë‚˜ì˜ ì •ë³´ê°€ í•¨ê»˜ ë‹´ê²¨ìˆë‹¤.
<div align=center>
<img src="../img/PolyEncoder/conv2ai_dataset.png" width=500/>
</div>
<br>

- DSTC7 dataset (Track 1)  
Ubuntu Chat logë¡œë¶€í„° ì¶”ì¶œí•œ ëŒ€í™” ë°ì´í„°   
DSTC7 challengeì—ì„œ ìš°ìŠ¹í•œ íŒ€ì€ 64.5% R@1ë¥¼ ë‹¬ì„±í•˜ì˜€ë‹¤.  

- Ubuntu V2  
ìœ„ì™€ ë¹„ìŠ·í•˜ì§€ë§Œ ë” í° ì‚¬ì´ì¦ˆì˜ corpusë¥¼ ê°€ì§    

**Article search in Information Retrieval**  
- Wiki Article Search   
ì•½ 5Mì˜ articleì„ í¬í•¨í•˜ë©°, ì–´ë–¤ article ë‚´ sentenceê°€ ì£¼ì–´ì§€ë©´ articleì„ ì°¾ëŠ” taskë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°ì´í„°ì…‹ì´ë‹¤  
ë‹¤ë¥¸ 1ë§Œê°œì˜ ê¸°ì‚¬ë“¤ ì‚¬ì´ì—ì„œ ì‹¤ì œ ê¸°ì‚¬ì˜ ìˆœìœ„ë¥¼ í†µí•´ ê²€ì¦í•œë‹¤.    

<div align=center>
ë°ì´í„°ì…‹ ë¹„êµ<br>
<img src="../img/PolyEncoder/table_1.png" width=800/>
</div>
<br>

### **4. Methods**

#### **4.1 Transformer and pre-training strategies**
#### **4.2 Bi-encoder**
#### **4.3 Cross-encoder**
#### **4.4 Poly-encoder**


### **5. Experiments**

### **6. Conclusion**

---

### **Appendix**  

#### **A. Training time**  

#### **B. Reduction layer in Bi-encoder**  

#### **C. Alternative choices for context vectors**