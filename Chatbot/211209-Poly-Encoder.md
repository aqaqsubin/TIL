## **Facebookì˜ Poly-Encoder ë…¼ë¬¸ ë¦¬ë·°** (12ì›” 7ì¼~12ì›” 9ì¼)

### [ğŸ“„**Paper**](https://openreview.net/pdf?id=SkxgnnNFvH)  
Humeau, S., Shuster, K., Lachaux, M. A., and Weston, J., â€œPoly-encoders: architectures and pre-training strategies for fast and accurate multi-sentence scoring,â€ _Proc. of the 8th International Conference on Learning Representations (ICLR 2020)_, Addis Ababa, Ethiopia, 2020.

### **ğŸ“Œ ëª©ì°¨** 

1. Introduction
2. Related work  
3. Tasks
4. Methods  
    4.1 Transformers and Pre-training Strategies  
    4.2 Bi-encoder  
    4.3 Cross-encoder  
    4.4 Poly-encoder
5. Experiments  
    5.1 Bi-encoders and Cross-encoders     
    5.2 Poly-encoders  
    5.3 Domain-specific pre-training  
    5.4 Inference speed  
6. Conclusion  

---

### **1. Introduction**

ê¸°ì¡´ì˜ pairwise comparision taskë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ Cross-encoder, Bi-encoder ì ‘ê·¼ë°©ì‹ì˜ ì„±ëŠ¥ ê°œì„ í•œ new Transformer architecture _Poly-encoder_ ì œì•ˆ  

**Poly-enocder**
- Cross-encoderì˜ ì¶”ë¡  ì‹œê°„ ê°œì„ 
- Bi-encoderì˜ prediction quality ê°œì„ 

**4ê°€ì§€ ë°ì´í„°ì…‹ì—ì„œì˜ Poly-encoderì„±ëŠ¥ ê²€ì¦**  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dialogue ë° Information Retrieval (IR) ë„ë©”ì¸ì— ëŒ€í•œ ë°ì´í„°ì…‹ì—ì„œ ì„±ëŠ¥ ê²€ì¦  

<br>

### **2. Related work**

1. Bi-encoder  
Inputê³¼ candidate labelì„ ê°™ì€ ê³µê°„ ìƒì— ë§¤í•‘í•œ í›„ ìœ ì‚¬ë„ë¥¼ ë¹„êµí•¨ìœ¼ë¡œì¨ scoring  
vector space model, LSI, supervised embeddings, classical siamese networks ë“±ì´ ìˆìŒ  
<br>
**next utterance prediction**ì—ë„ Memory Networks, Transformer Memory networks, LSTMs, CNNsì™€ ê°™ì´   
inputê³¼ candidate labelì„ ë”°ë¡œ ì¸ì½”ë”©í•˜ëŠ” Bi-encoder approachê°€ ì‚¬ìš©ë˜ì—ˆë‹¤.   
<br>
inputê³¼ candidatesë¥¼ ê°ê° ë”°ë¡œ ì„ë² ë”©í•˜ê¸° ë•Œë¬¸ì—, candidatesë¥¼ ì €ì¥í•˜ì—¬ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤.  
â†’ candidateë¥¼ ë‹¤ì‹œ ì„ë² ë”©í•  í•„ìš”ê°€ ì—†ê¸° ë•Œë¬¸ì— ì¶”ë¡  ì‹œê°„ì´ ë¹¨ë¼ì§

2. Cross-encoder  
Inputê³¼ candidateë¥¼ ì´ì–´ë¶™ì—¬(concatenation) í•˜ë‚˜ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ë©°, non-linear functionì— ì˜í•´ scoring ëœë‹¤.  
Sequential Matching Network CNN-based architecture, Deep Matching Networks, Gated Self-Attention ë“±ì´ ìˆë‹¤.    
<br>
ê°€ì¥ ìµœì‹  transformerë¥¼ ì´ìš©í•œ Cross-encoder approachëŠ” ê° layerì— self-attentionì„ ì ìš©í•œ ê²°ê³¼ë¥¼ ë‚´ë©°,   
inputê³¼ candidate ê°„ì˜ interactionì„ ê·¹ëŒ€í™”í•œë‹¤. (candidate ë‚´ wordëŠ” input ë‚´ ëª¨ë“  wordì— attend í•  ìˆ˜ ìˆìŒ)  

> Urbanek et al. (2019)ì˜ ì—°êµ¬ì—ì„œëŠ” ì‚¬ì „í•™ìŠµëœ BERTë¥¼ í†µí•œ Cross-encoderì™€ Bi-encoderì˜ ì„±ëŠ¥ì„ ë¹„êµí–ˆìœ¼ë©°,  
Cross-encoderì˜ ì ‘ê·¼ ë°©ì‹ì´ ì„±ëŠ¥ì´ ë” ë›°ì–´ë‚˜ì§€ë§Œ ì¶”ë¡  ì‹œê°„ ë©´ì—ì„œëŠ” ë’¤ì³ì¡Œë‹¤.

<br>

### **3. Tasks**

ë‹¤ìŒ ë‘ ê°€ì§€ ë„ë©”ì¸ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì…‹ì„ í†µí•´ ë³¸ ì—°êµ¬ì—ì„œ ì œì•ˆí•˜ëŠ” Poly-encoder ì„±ëŠ¥ì„ ê²€ì¦í•˜ì˜€ë‹¤.  

**Sentence Selection in Dialogue**

- ConvAI2 dataset  
ëŒ€í™” ë°ì´í„°ë¡œ, ê° ë°œí™”ìì— ëŒ€í•œ í˜ë¥´ì†Œë‚˜ì˜ ì •ë³´ê°€ í•¨ê»˜ ë‹´ê²¨ìˆë‹¤.
<div align=center>
<img src="../img/PolyEncoder/conv2ai_dataset.png" width=500/>
</div>
<br>

- DSTC7 dataset (Track 1)  
Ubuntu Chat logë¡œë¶€í„° ì¶”ì¶œí•œ ëŒ€í™” ë°ì´í„°   
DSTC7 challengeì—ì„œ ìš°ìŠ¹í•œ íŒ€ì€ 64.5% R@1ë¥¼ ë‹¬ì„±í•˜ì˜€ë‹¤.  

- Ubuntu V2  
ìœ„ì™€ ë¹„ìŠ·í•˜ì§€ë§Œ ë” í° ì‚¬ì´ì¦ˆì˜ corpusë¥¼ ê°€ì§    

**Article search in Information Retrieval**  
- Wiki Article Search   
ì•½ 5Mì˜ articleì„ í¬í•¨í•˜ë©°, ì–´ë–¤ article ë‚´ sentenceê°€ ì£¼ì–´ì§€ë©´ articleì„ ì°¾ëŠ” taskë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°ì´í„°ì…‹ì´ë‹¤  
ë‹¤ë¥¸ 1ë§Œê°œì˜ ê¸°ì‚¬ë“¤ ì‚¬ì´ì—ì„œ ì‹¤ì œ ê¸°ì‚¬ì˜ ìˆœìœ„ë¥¼ í†µí•´ ê²€ì¦í•œë‹¤.    

<div align=center>
ë°ì´í„°ì…‹ ë¹„êµ<br>
<img src="../img/PolyEncoder/table_1.png" width=800/>
</div>
<br>

### **4. Methods**

#### **4.1 Transformer and pre-training strategies**  
ë³¸ ë…¼ë¬¸ì—ì„œ ì†Œê°œí•˜ëŠ” Bi-encoder, Cross-encoder, Poly-encoderëŠ” large pre-trained Transformerì— ê¸°ë°˜í•˜ê³  ìˆë‹¤. 

**ì‚¬ìš©í•œ ì‚¬ì „ í•™ìŠµ ëª¨ë¸**
- BERT-base on _Wikipedia & Toronto Books Corpus_   
Wikipedia & Toronto Books Corpusì˜ ì´ 1ì–µ 5ì²œë§Œê°œì˜ [INPUT, LABEL] ìŒì˜ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ 
<u>Masked Language Model(MLM)</u> taskì™€ <u>Next Sentence Prediction(NSP)</u> task í•™ìŠµ  

- BERT-base on _Reddit_  
Redditì˜ ì´ 1ì–µ 7ì²œë§Œê°œì˜ [INPUT, LABEL]ìŒì˜ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬  
<u>Masked Language Model(MLM)</u> taskì™€ <u>Next Utterance Prediction(NUP)</u> task í•™ìŠµ  
(_Redditì´ dialogue taskì— ë” ì í•©_)

> **Next Sentence Prediction**  
í›„ë³´êµ°ì˜ 50%ëŠ” ì§„ì§œ next sentenceë¡œ êµ¬ì„±í•˜ê³  ë‚˜ë¨¸ì§€ 50%ëŠ” ëœë¤í•˜ê²Œ ì„ íƒëœ sentenceë¡œ êµ¬ì„±í•˜ì—¬ ì œê³µ  
í›„ë³´êµ° ì¤‘ì—ì„œ next sentenceë¥¼ ì°¾ëŠ” task  
>
> **Next Utterance Prediction**  
> utteranceê°€ í•˜ë‚˜ ì´ìƒì˜ ë¬¸ì¥ìœ¼ë¡œ ì´ë£¨ì–´ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ì ì—ì„œ NSPì™€ ì•½ê°„ ë‹¤ë¥´ë‹¤


#### **4.2 Bi-encoder**  
input contextì™€ candidate labelì„ ë”°ë¡œ ì¸ì½”ë”©í•˜ëŠ” êµ¬ì¡°  
<div align=center>
<img src="../img/PolyEncoder/bi-encoder.jpeg" width=500/>
</div>
<br>

- Input $ctxt, cand$  
Nê°œì˜ í† í°ìœ¼ë¡œ í† í°í™”ëœ input contextì™€ candidate label  

- Encoder $T_1, T_2$    
ê°™ì€ ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì§€ë§Œ, fine-tuningì˜ parameter upadateëŠ” ë”°ë¡œ ì§„í–‰  

- Output of Encoder $T(x)=h_1,..,h_N$   
(N, 768) (768 is *Embedding size*) í¬ê¸°ë¡œ ì¸ì½”ë”©ëœ ë²¡í„°

- Aggregator $red(\cdot)$  
Nê°œì˜ ë²¡í„°ë¥¼ í•˜ë‚˜ì˜ representationìœ¼ë¡œ í‘œí˜„í•˜ê¸° ìœ„í•¨ì´ë‹¤. 
(N, 768) â†’ (1, 768)  

    1. ì²« ë²ˆì§¸ ë²¡í„° ì„ íƒ  
    2. mê°œ ë²¡í„°ë“¤ì˜ í‰ê·  ë²¡í„° ($m<N$)
    3. Nê°œ ë²¡í„°ë“¤ì˜ í‰ê·  ë²¡í„°

- Scoring function $s(ctxt, cand_i)$  
dot productë¥¼ í†µí•´ ië²ˆì§¸ í›„ë³´êµ°ì— ëŒ€í•œ ì ìˆ˜ë¥¼ ê³„ì‚°í•œë‹¤  

$$s(ctxt, cand_i) = y_{ctxt}\cdot y_{cand_i}$$

> **Bi-encoder ì¥ì  ğŸŒŸ**  
> ëª¨ë“  candidatesì— ëŒ€í•œ ì„ë² ë”©ì„ ì €ì¥í•˜ì—¬ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—,  
> ì¶”ë¡  ì‹œê°„ ë©´ì—ì„œ íš¨ìœ¨ì ì´ë‹¤  
>

#### **4.3 Cross-encoder**
input contextì™€ candidate labelì„ ì´ì–´ë¶™ì—¬(concatenate) ìƒˆë¡œìš´ inputìœ¼ë¡œ ì‚¬ìš©  
<div align=center>
<img src="../img/PolyEncoder/cross-encoder.jpeg" width=500/>
</div>
<br>

- Input $ctxt, cand$  
2*Nê°œì˜ í† í°ìœ¼ë¡œ í† í°í™”ëœ $[ctxt, cand]$ 

- Encoder $T$    
í•˜ë‚˜ì˜ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸ì½”ë”©

- Output of Encoder $T(ctxt, cand)$   
768 í¬ê¸°ë¡œ ì¸ì½”ë”©ëœ 2*Nê°œì˜ ë²¡í„°

- Aggregator $first(\cdot)$  
2*Nê°œì˜ ë²¡í„°ë¥¼ í•˜ë‚˜ì˜ representation $y_{ctxt,cand}$ìœ¼ë¡œ í‘œí˜„í•˜ê¸° ìœ„í•¨ì´ë‹¤.   
ì²«ë²ˆì§¸ ë²¡í„°ë¥¼ ì¶”ì¶œí•œë‹¤

$$y_{ctxt,cand} = first(T(ctxt,cand))$$

- Dim Reduction (Scoring function, $s(ctxt, cand_i)$)  
ì„ í˜• ë ˆì´ì–´ $W$ë¥¼ ê±°ì³ ië²ˆì§¸ í›„ë³´êµ°ì— ëŒ€í•œ ì ìˆ˜ë¥¼ ê³„ì‚°í•œë‹¤  

$$s(ctxt, cand_i) = y_{ctxt},y_{cand_i}W$$

> **Cross-encoder ì¥ì  ğŸŒŸ**  
> Transformerì— ê¸°ë°˜í•œ ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì˜€ê¸° ë•Œë¬¸ì—,  
> input contextì™€ candidate labelì„ ì´ì–´ë¶™ì¸ ìƒˆë¡œìš´ ì…ë ¥ì— self-attentionì„ ì ìš©í•˜ëŠ” êµ¬ì¡°ë¥¼ ë¤ë‹¤.  
> <u>candidate-sensitive input representation</u>ì„ ë§Œë“¤ì–´ Bi-encoderë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ë‹¤  
>
> **Cross-encoder ë‹¨ì  â˜”**  
> Bi-encoderì™€ ë‹¬ë¦¬ candidateì˜ ì„ë² ë”©ì„ ì €ì¥í•˜ê³  ì¬ì‚¬ìš©í•  ìˆ˜ ì—†ë””.  
> inputê³¼ candidateì„ ì´ì–´ë¶™ì¸ ì…ë ¥ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ í¬ë‹¤. â†’ batch sizeë¥¼ ì¤„ì„  
> ì¶”ë¡  ì‹œê°„ì´ ëŠë¦¬ë‹¤.

#### **4.4 Poly-encoder**
Bi-encoderì˜ ì„±ëŠ¥ì„ ê°œì„ í•˜ë©´ì„œ Cross-encoderì˜ ë‹¨ì ì„ ê·¹ë³µí•œ í”„ë ˆì„ì›Œí¬

<div align=center>
<img src="../img/PolyEncoder/poly-encoder.png" width=500/>
</div>
<br>

- Input $ctxt, cand$  
Nê°œì˜ í† í°ìœ¼ë¡œ í† í°í™”ëœ input contextì™€ candidate label  

- Encoder $T_1, T_2$    
ê°™ì€ ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì§€ë§Œ, fine-tuningì˜ parameter upadateëŠ” ë”°ë¡œ ì§„í–‰  

- Output of Encoder $T(x)=h_1,..,h_N$   
(N, 768) (768 is *Embedding size*) í¬ê¸°ë¡œ ì¸ì½”ë”©ëœ ë²¡í„°

- Aggregator $red(\cdot)$  
Nê°œì˜ ë²¡í„°ë¥¼ í•˜ë‚˜ì˜ representation $y_{cand_i}$ìœ¼ë¡œ í‘œí˜„í•˜ê¸° ìœ„í•¨ì´ë‹¤.   

$$y_{cand_i} = red(T_2(cand_i))$$

- Attention
    - mê°œì˜ Global feature ìƒì„±  
    mê°œì˜ context code $(c_1,...,c_m)$ë¥¼ ì •ì˜í•˜ì—¬ global feature $y^i_{ctxt}$ë¥¼ ì¶”ì¶œ   
    <br>
    êµ¬í˜„ ì‹œ, context codes $(c_1,...,c_m)$ì€ ëœë¤í•˜ê²Œ ì´ˆê¸°í™”ëœ íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµì‹œì¼œ ì‚¬ìš©í•˜ì˜€ë‹¤.   
    [ì¶œì²˜: facebookresearch](https://github.com/facebookresearch/ParlAI/blob/master/parlai/agents/transformer/polyencoder.py#L342)
 
    $$(w^{C_i}_1,..,w^{C_i}_N)=softmax(c_i\cdot h_1,.., c_i\cdot h_N)$$
    $$y^i_{ctxt}=\sum_{j=1}^{N}w^{C_i}_jh_j$$ 

    - Context-Candidate Attention  
    ì£¼ì–´ì§„ mê°œì˜ global feature vectorì— $y_{cand_i}$ë¥¼ attend í•¨ìœ¼ë¡œì¨,  
    cross-encoderì²˜ëŸ¼ candidateê³¼ input context ê°„ì˜ ë” ë§ì€ interactionì„ ì¶”ì¶œí•˜ê¸° ìœ„í•¨ì¸ê²ƒ ê°™ë‹¤ 
 
    $$(w_1,..,w_m)=softmax(y_{cand_i}\cdot y^1_{ctxt},.., y_{cand_i}\cdot y^m_{ctxt})$$
    $$y_{ctxt}=\sum_{i=1}^{m}w_iy^i_{ctxt}$$ 


> **Poly-encoder ì¥ì  ğŸŒŸ**  
> Bi-encoderì²˜ëŸ¼ input contextì™€ candidateì„ ë”°ë¡œ ì¸ì½”ë”©í•¨ìœ¼ë¡œì¨ candidate ì„ë² ë”©ì„ ì €ì¥í•  ìˆ˜ ìˆë‹¤.  
> Cross-encoderëŠ” input contextì™€ candidateì— attentionì„ ëª¨ë“  ê³„ì¸µë§ˆë‹¤ ìˆ˜í–‰í•˜ëŠ” ë°˜ë©´,  
(ì¸ì½”ë”ì˜ ëª¨ë“  ê³„ì¸µì—ì„œ attention ìˆ˜í–‰)  
> Poly-encoderëŠ” ë§ˆì§€ë§‰ ê³„ì¸µì—ì„œë§Œ candidateê³¼ input context ê°„ì˜ attentionì„ ìˆ˜í–‰í•˜ë¯€ë¡œ ë” ë¹ ë¥´ë‹¤

[Poly-encoder, Bi-encoder, Cross-encoder êµ¬í˜„ğŸ‘Š](https://github.com/aqaqsubin/Pairwise-Comparison-Model/blob/)  
ë…¼ë¬¸ì„ ë³´ë©° êµ¬í˜„í•´ë³¸ ì½”ë“œë¡œ í‹€ë¦´ ìˆ˜ ìˆë‹¤.. 

(Poly-encoder ì½”ë“œëŠ” facebookresearch repoì—ì„œ ì˜¤í”ˆì†ŒìŠ¤ë¡œ ê³µê°œë˜ì–´ìˆë‹¤.)

<br>

### **5. Experiments**
ì¸¡ì • ë©”íŠ¸ë¦­ : Recall at k(Recall@k), Mean Reciprocal Rank(MRR)

- R@k/C : ì „ì²´ Cê°œì˜ í›„ë³´ë“¤ ì¤‘ kê°œë¥¼ ì„ íƒí–ˆì„ ë•Œ, ì‹¤ì œ relevant í›„ë³´êµ° ëŒ€ë¹„ ì„ íƒëœ relevant í›„ë³´êµ° ë¹„ìœ¨  
- MRR : relevantí•œ í›„ë³´êµ°ì´ ì–¼ë§ˆë‚˜ ìƒìœ„ì— rankingë˜ì–´ ìˆëŠ”ì§€ íŒë‹¨  
    relevantí•œ í›„ë³´êµ°ì˜ ê° ìˆœìœ„ë¥¼ ì—­ìˆ˜ë¡œ í•˜ì—¬ í‰ê· ì„ êµ¬í•œ ê°’ì´ë‹¤

#### **5.1 Bi-encoders and Cross-encoders**

<div align=center>
<img src="../img/PolyEncoder/table_2.png" width=400/><br>
negatives ìˆ˜ì— ë”°ë¥¸ ì„±ëŠ¥ ê²€ì¦
</div>
<br>

negativeê°€ ì¦ê°€í•  ìˆ˜ë¡ ì„±ëŠ¥ì´ í–¥ìƒëœë‹¤.   
Bi-encoderëŠ” negative ìˆ˜ë¥¼ ëŠ˜ë¦´ ìˆ˜ ìˆì§€ë§Œ Cross-encoderëŠ” ê·¸ëŸ´ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—  
DSTC7 and Ubuntu V2, ConvAI2 ë°ì´í„°ì…‹ì— ëŒ€í•´ ê°ê° 15, 19ê°œì˜ negativeë¥¼ ì‚¬ìš©í•œë‹¤  

<div align=center>
<img src="../img/PolyEncoder/table_3.png" width=800/><br>
BERT-base ëª¨ë¸ì˜ fine-tuningì— ë”°ë¥¸ ì„±ëŠ¥ ë¶„ì„
</div>
â†’ word-embeddingì„ ì œì™¸í•œ ëª¨ë“  layer fine-tuning  

<br><br>

<div align=center>
<img src="../img/PolyEncoder/table_4.png" width=1000/><br>
ConvAI2, DSTC7, Ubuntu V2, Wikipedia IR 4ê°€ì§€ taskì— ëŒ€í•œ ì„±ëŠ¥ ë¹„êµ
</div>
<br>

1. Bi-encoderì™€ Cross-encoderëŠ” ConvAI2, DSTC7, Ubuntu V2ì— ëŒ€í•´ ê¸°ì¡´ì— ì œì•ˆëœ ì ‘ê·¼ ë°©ë²•ì„ ëŠ¥ê°€í–ˆë‹¤  
2. Cross-encoderì˜ ì„±ëŠ¥ì´ ê°€ì¥ ë›°ì–´ë‚˜ë‹¤

#### **5.2 Poly-encoders**

Poly-encoderì—ì„œëŠ” mê°œì˜ context codeë¥¼ í†µí•´ input contextë¡œë¶€í„° global featureë¥¼ ì¶”ì¶œí•˜ëŠ”ë°,  
ìœ„ì˜ table 4ì—ì„œëŠ” mì˜ í¬ê¸°ì— ë”°ë¥¸ ì„±ëŠ¥ë„ ë¶„ì„í•˜ì˜€ë‹¤.

mì´ ì»¤ì§ˆìˆ˜ë¡ ì„±ëŠ¥ì´ í–¥ìƒë¨ì„ ì•Œ ìˆ˜ ìˆë‹¤. 

> Li et al.(2019)ì˜ ì—°êµ¬ì—ì„œëŠ” ConvAI2 ë°ì´í„°ì…‹ì— ëŒ€í•´ human evaluationì„ ìˆ˜í–‰í•œ ê²°ê³¼,  
Poly-encoderê°€ ê°€ì¥ ë›°ì–´ë‚¬ë‹¤ê³  ë³´ê³ í–ˆë‹¤

#### **5.3 Domain-specific pre-training**

ë³¸ ì—°êµ¬ì—ì„œ ì œì•ˆí•œ 3ê°€ì§€ ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì¤‘ì—ì„œ, Reddit ë°ì´í„°ë¡œ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì´ SOTA ë‹¬ì„±

Redditì´ ëŒ€í™” ë°ì´í„°ì— ë” ê°€ê¹Œì› ê³ , finetuningìœ¼ë¡œ í•˜ê³ ì í•˜ëŠ” staskì— ê°€ê¹Œì› ë‹¤.  
> fine-tuningìœ¼ë¡œ í•˜ê³ ì í•˜ëŠ” taskì™€ ë¹„ìŠ·í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ pretraining taskë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ ê°œì„ ìœ¼ë¡œ ì´ì–´ì§„ë‹¤.

#### **5.4 Inference speed**

<div align=center>
<img src="../img/PolyEncoder/table_5.png" width=1000/><br>
100ê°œ dialogue examples í‰ê·  ì¶”ë¡  ì‹œê°„ ë¹„êµ
</div>
<br>

- 1kê°œì˜ í›„ë³´êµ°ì„ ê°€ì§ˆ ë•Œ Bi-encoderì™€ Poly-encoderê°„ì˜ ì¶”ë¡  ì‹œê°„ ì°¨ì´ê°€ ì ë‹¤
- Bi-encoderì™€ Poly-encoderì— ë¹„í•´ Cross-encoderì˜ ì¶”ë¡  ì‹œê°„ì€ ë„ˆë¬´ ê¸¸ë‹¤

<br>

### **6. Conclusion**

candidate embeddingì„ ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬ ì €ì¥í•  ìˆ˜ ìˆëŠ” Bi-encoderì˜ ì¥ì ì„ ìœ ì§€í•œ ì±„,
context representationì— candidate labelì„ attendingí•˜ëŠ” Cross-encoderì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì˜€ë‹¤.  

ì¶”ë¡  ì‹œê°„ê³¼ ì •í™•ë„ì˜ ì ì ˆí•œ trade-offê°€ ì´ë£¨ì–´ì§„ Poly-encoder ëª¨ë¸ ì œì•ˆ!

---

### **Appendix**  

#### **A. Training time**  
<div align=center>
<img src="../img/PolyEncoder/table_6.png" width=1000/><br>
3ê°€ì§€ ë°ì´í„°ì…‹ì— ëŒ€í•œ ëª¨ë¸ë“¤ì˜ í›ˆë ¨ì‹œê°„ ë¹„êµ
</div>
<br>

#### **B. Reduction layer in Bi-encoder**  

<div align=center>
<img src="../img/PolyEncoder/table_7.png" width=1000/><br>
Aggregator 3ê°€ì§€ ë°©ë²•ì— ëŒ€í•œ Bi-encoder ì„±ëŠ¥ ë¹„êµ
</div>
<br>

ì²«ë²ˆì§¸ ë²¡í„°ë¥¼ ì‚¬ìš©í•˜ëŠ”ê²Œ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ë‹¤


#### **C. Alternative choices for context vectors**

<div align=center>
<img src="../img/PolyEncoder/table_8.png" width=700/><br>
mì˜ í¬ê¸° ë° mê°œì˜ global featureë¥¼ ì •ì˜í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ì„±ëŠ¥ ë¹„êµ
</div>
<br>

- Learnt-codes : context codesë¥¼ ì •ì˜í•˜ì—¬ ëª¨ë“  output $h^1_{ctxt},..,h^N_{ctxt}$ì— attendingí•˜ì—¬ $y^i_{ctxt}$ë¥¼ ì¶”ì¶œí•˜ë„ë¡ í•™ìŠµì‹œí‚¨ ë°©ë²•

<div align=center>
<img src="../img/PolyEncoder/learnt-m.png" width=600/>
</div>
<br>

- First m outputs : ì²˜ìŒ mê°œì˜ outputì„ ì‚¬ìš©
- Last m outputs : ë§ˆì§€ë§‰ mê°œì˜ outputì„ ì‚¬ìš©
- Last m outputs and $h^1_{ctxt}$ : BERTì˜ special token [S]ì— í•´ë‹¹í•˜ëŠ” ì²«ë²ˆì§¸ output $h^1_{ctxt}$ì™€ ë§ˆì§€ë§‰ mê°œì˜ outputì„ ì´ì–´ë¶™ì—¬ ì‚¬ìš©

<div align=center>
<img src="../img/PolyEncoder/first-m.png" width=600/>
</div>
<br>


<div align=center>
<img src="../img/PolyEncoder/table_9.png" width=800/>
</div>
<br>

mì˜ í¬ê¸°ê°€ í´ìˆ˜ë¡ ì„±ëŠ¥ì´ í–¥ìƒë¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.
mì´ ì»¤ì§„ë‹¤ë©´ learnt-më³´ë‹¤ first-mì„ ì‚¬ìš©í•˜ëŠ”ê²Œ ì¶”ë¡  ì‹œê°„ ë©´ì—ì„œ ìœ ë¦¬í•  ìˆ˜ë„ ìˆê² ë‹¤!