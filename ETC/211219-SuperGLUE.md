# ğŸ—½ **SuperGLUE**

ìì—°ì–´ ì´í•´ ì‹œìŠ¤í…œì„ í›ˆë ¨ì‹œí‚¤ê³ , ê·¸ ì„±ëŠ¥ì„ í‰ê°€ ë° ë¹„êµ ë¶„ì„í•˜ê¸° ìœ„í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ êµ¬ì„±

### **1. BoolQ**

(question, passage, answer)ì˜ tripletìœ¼ë¡œ êµ¬ì„±ë˜ë©°,   
ëª¨ë¸ì— questionê³¼ passageê°€ ì£¼ì–´ì§€ë©´, yes/noë¡œ QAë¥¼ ìˆ˜í–‰ 

> **BoolQ ë°ì´í„° ì˜ˆì‹œ**
> 
> 
> <div align=left><img src="../img/GLUE/boolq_example.png" width=400/></div>
> 

<br>

### **2. CB (CommitmentBank)**

MNLIì™€ ë¹„ìŠ·í•œ í…ŒìŠ¤í¬ë¥¼ ìˆ˜í–‰í•˜ë©°, ê° *premise*ëŠ” embedded clauseë¥¼ í•˜ë‚˜ ì´ìƒ í¬í•¨í•œë‹¤.

> **CB ë°ì´í„° ì˜ˆì‹œ**  
> 
> <div align=left><img src="../img/GLUE/cb_example.png" width=400/></div>
> 

<br>

### **3. COPA (Choice of Plausible Alternatives)**

premiseê°€ ì£¼ì–´ì§€ê³ , 2ê°œì˜ ì„ íƒì§€ì—ì„œ premiseì˜ ì›ì¸ ë˜ëŠ” ê²°ê³¼ë¥¼ ì„ íƒí•˜ëŠ” í…ŒìŠ¤í¬

<br>

### **4. MultiRC (Multi-Sentence Reading Comprehension)**

context paragraphì™€ ê·¸ì— ëŒ€í•œ question, ê·¸ë¦¬ê³  í›„ë³´ ë‹µì•ˆì´ ì£¼ì–´ì¡Œì„ ë•Œ ì •ë‹µì„ ì°¾ëŠ” QA task

> MultiRC (Multi-Sentence Reading Comprehension, Khashabi et al., 2018) is a QA task where each example consists of a context paragraph, a question about that paragraph, and a list of possible answers. The system must predict which answers are true and which are false.
> 

<br>

### **5. ReCoRD (Reading Comprehension with Commonsense Reasoning Dataset)**

articleê³¼ ê·¸ì— ëŒ€í•œ questionìœ¼ë¡œ êµ¬ì„±ë˜ë©°, question ë‚´ entityê°€ masking ë˜ì–´ ìˆë‹¤.    
í›„ë³´ entity ì¤‘ masking ëœ entityê°€ ë¬´ì—‡ì„ ì§€ì¹­í•˜ëŠ”ì§€ ì„ íƒí•˜ëŠ” task

> ReCoRD (Reading Comprehension with Commonsense Reasoning Dataset, Zhang et al., 2018) is a multiple-choice QA task. Each example consists of a news article and a Cloze-style question about the article in which one entity is masked out. The system must predict the masked out entity from a list of possible entities in the provided passage, where the same entity may be expressed with multiple different surface forms, which are all considered correct.
> 

<br>

> **ReCoRD ë°ì´í„° ì˜ˆì‹œ**
> 
> <div align=left><img src="../img/GLUE/record_example.png" width=700/></div>
> 

<br>

### **6. RTE (Recognizing Textual Entailment)**

í…ìŠ¤íŠ¸ì™€ hypothesisê°€ ì£¼ì–´ì¡Œì„ ë•Œ, í•œ í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ê°€ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ì— í¬í•¨ë˜ëŠ” ì§€ íŒë‹¨í•˜ê¸° ìœ„í•œ ë°ì´í„°ì…‹ 

> The RTE task is defined as recognizing, given two text fragments, whether the meaning of one text can be inferred (entailed) from the other.
> 

<br>

### **7. WiC (Words in Context)**
Context ë‚´ ë‹¨ì–´ ê°„ ì˜ë¯¸ê°€ ê°™ì€ì§€ íŒë‹¨

> **WiC ë°ì´í„° ì˜ˆì‹œ**
> 
> <div align=left><img src="../img/GLUE/wic_example.png" width=700/></div>
> 

<br>

### **8. WSC (The Winograd Schema Challenge)**
ë‘ ë‹¨ì–´ê°€ ê°™ì€ ëŒ€ìƒì„ ì§€ì¹­í•˜ëŠ”ì§€ íŒë‹¨

> **WSC ë°ì´í„° ì˜ˆì‹œ**
> 
>  <div align=left><img src="../img/GLUE/wsc_example.png" width=700/></div>
>