#  ğŸ—½ **GLUE : General Language Understanding Evaluation**

ìì—°ì–´ ì´í•´ ì‹œìŠ¤í…œì„ í›ˆë ¨ì‹œí‚¤ê³ , ê·¸ ì„±ëŠ¥ì„ í‰ê°€ ë° ë¹„êµ ë¶„ì„í•˜ê¸° ìœ„í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ êµ¬ì„±

### **1. CoLA (The Corpus of Linguistic Acceptability)**

ì£¼ì–´ì§„ ë¬¸ë²•ì ìœ¼ë¡œ ìˆ˜ìš© ê°€ëŠ¥í•œì§€ íŒë‹¨í•˜ê¸° ìœ„í•œ ë°ì´í„°ì…‹

> This paper investigates the ability of artificial neural networks to judge the grammatical acceptability of a sentence, with the goal
of testing their linguistic competence.
> 

<br>

### **2. QQP (Quora Question Pairs)**

ë‘ ê°œì˜ ì§ˆë¬¸ì´ ê°™ì€ ì˜ë„ë¥¼ ê°€ì§€ëŠ”ì§€ íŒë‹¨í•˜ê¸° ìœ„í•œ ë°ì´í„°ì…‹

<br>

### **3. MNLI(Multi-Genre NLI)**

ì „ì œ ë¬¸ì¥ê³¼ hypothesisê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ë‘˜ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ íŒë‹¨í•˜ê¸° ìœ„í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ, 

í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ ì¥ë¥´ì˜ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ Stanford NLI Corpusì™€ ë‹¬ë¦¬ ì—¬ëŸ¬ ì¥ë¥´ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì˜€ë‹¤.

> a model is presented with a pair of sentences and asked to judge the relationship between their meanings by picking a label from
a small set: typically **ENTAILMENT**, **NEUTRAL**, and **CONTRADICTION**.
> 

> There are five unique prompts in total: one for written non-fiction genres (SLATE, OUP, GOVERNMENT, VERBATIM, TRAVEL; Figure 1), one for spoken genres (TELEPHONE, FACE-TO-FACE), one for each of the less formal written genres (FICTION, LETTERS), and a specialized one for 9/11, tailored to fit its potentially emotional content.
> 

<br>

**3.1 MNLI-m (MultiNLI Matched)**

ëª¨ë¸ í›ˆë ¨ ì‹œ 5ê°œì˜ ì¥ë¥´ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ë©° (*FICTION, GOVERNMENT, SLATE, TELEPHONE, TRAVEL*)

ì´ 5ê°œì˜ ì¥ë¥´ì— ì†í•˜ëŠ” ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸í•¨

> All of the genres appear in the test and development sets, but only five are included in the training set. Models thus can be evaluated on both the ***matched*** test examples, which are derived from the same sources as those in the training set, and on the ***mismatched*** examples, which do not closely resemble any of those seen at training time.
> 

<br>

**3.2 MNLI-mm (MultiNLI Mismatched)**

ìœ„ 5ê°œì˜ ì¥ë¥´ì— ì†í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸(*9/11, FACE-TO-FACE, LETTERS, OUP, VERBATIM*)

<br>

### **4. SST (The Stanford Sentiment Treebank)**

ë¬¸ì¥ì˜ ê°ì • ë¶„ë¥˜ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ,  
ë¬¸ì¥ì´ íŠ¸ë¦¬êµ¬ì¡°ë¡œ í‘œí˜„ë˜ì–´ ìˆê³  ê° ë…¸ë“œì— ëŒ€í•´ ê¸ì •ì˜ ì •ë„ì— ë”°ë¼ 0~4ë¡œ ë ˆì´ë¸”ë§ì„ ìˆ˜í–‰í•˜ì˜€ë‹¤.  


> <div align=left><img src="../img/GLUE/sst_example.png" width=500/></div>
> 
> S. W. Lee, â€œSentiment analysis system using stanford sentiment treebank,â€ *Journal of the Korean Society of Marine Engineering*, vol. 39, no. 3, pp. 274~279, 2015. (in Korean)


<br>

### **5. STS-B (Semantic Textual Similarity Benchmark)**

ì£¼ì–´ì§„ ë‘ í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ê°€ ìœ ì‚¬í•œì§€ íŒë‹¨í•˜ê¸° ìœ„í•œ ë°ì´í„°ì…‹

<br>

### **6. QNLI (Question NLI)**

paragraph-question pairë¡œ êµ¬ì„±

paragraph ë‚´ ê° ë¬¸ì¥ì„ context sentenceë¼ í•  ë•Œ  ì£¼ì–´ì§„ questionì— ëŒ€í•œ ë‹µì´ context sentenceì— í¬í•¨ë˜ì—ˆëŠ”ì§€ íŒë³„

*ì£¼ì–´ì§„ questionì— ëŒ€í•œ answerëŠ” paragraph ë‚´ì— ì¡´ì¬í•œë‹¤*

> We convert the task into sentence pair classification by forming a pair between each question and each sentence in the corresponding context, and filtering out pairs with low lexical overlap between the question and the context sentence. The task is to determine whether the context sentence contains the answer to the question
> 

<br>

### **7. RTE (Recognizing Textual Entailment)**

í…ìŠ¤íŠ¸ì™€ hypothesisê°€ ì£¼ì–´ì¡Œì„ ë•Œ, í•œ í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ê°€ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ì— í¬í•¨ë˜ëŠ” ì§€ íŒë‹¨í•˜ê¸° ìœ„í•œ ë°ì´í„°ì…‹ 

> The RTE task is defined as recognizing, given two text fragments, whether the meaning of one text can be inferred (entailed) from the other.
> 

<br>

### **8. MRPC (Microsoft Research Paraphrase Corpus)**

ë‘ ê°œì˜ í…ìŠ¤íŠ¸ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, í•œ í…ìŠ¤íŠ¸ê°€ ë‹¤ë¥¸ ë¬¸ì¥ì„ íŒ¨ëŸ¬í”„ë ˆì´ì§•í•œ ê²ƒì¸ì§€ ì•„ë‹Œì§€ íŒë‹¨í•˜ê¸° ìœ„í•œ ë°ì´í„°ì…‹

<br>

---

â•Â ë…¼ë¬¸ ë‚´ ì‹¤í—˜ì— í¬í•¨ë˜ì§€ ì•Šì€ í…ŒìŠ¤í¬ 

<br>

### **9. WNLI(Winograd NLI)**

ëŒ€ëª…ì‚¬ê°€ í¬í•¨ëœ ë¬¸ì¥ì´ ì£¼ì–´ì§€ê³ , ê·¸ ëŒ€ëª…ì‚¬ê°€ ê°€ë¦¬í‚¤ëŠ” ëŒ€ìƒì„ ì„ íƒí•˜ê¸° ìœ„í•œ ë°ì´í„°ì…‹

> The Winograd Schema Challenge (Levesque et al., 2011) is a reading comprehension task in which a system must read a sentence with a pronoun and select the referent of that pronoun from a list of choices.
> 

train-devì˜ ë°ì´í„° ë¶„í¬ ì°¨ì´ì˜ ë¬¸ì œë¡œ ì¸í•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ í•˜ë½í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— BERTì—ì„œëŠ” ì´ë¥¼ ì œì™¸í•¨

> The GLUE webpage notes that there are issues with the construction of this dataset, 15 and every trained system thatâ€™s been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class. We therefore exclude this set to be fair to OpenAI GPT.
>