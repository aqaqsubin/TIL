# **Support Vector Machine**

**Decision Boundary:** ë¶„ë¥˜ë¥¼ ìœ„í•œ ê¸°ì¤€ hyperplane


### Q. ì–´ë–¤ Decision Boundaryê°€ ì¢‹ì€ ê²ƒì¸ê°€?  
### A. marginì„ ìµœëŒ€í™”í•˜ëŠ” Decision Boundary

<br>

**Support Vector**: ëª¨ë“  ë°ì´í„°ë“¤ì´ boundaryì— ìˆ˜ì„ ì˜ ë°œì„ ë‚´ë ¸ì„ ë•Œ,

boundaryì™€ ê°€ì¥ ê°€ê¹Œìš´ ê±°ë¦¬ë¥¼ ê°€ì§„ ë²¡í„°

**Margin**:   
~ boundaryì™€ support vector ê°„ì˜ ê±°ë¦¬  
~ boundaryì— ê°€ì¥ ê°€ê¹Œìš´ point ê°„ì˜ ê±°ë¦¬

![SVM](../img/Machine-Learning/svm.png)

### **â‡’ marginì€ ì–´ë–»ê²Œ ìœ ë„í• ê¹Œ**

ìš°ë¦¬ê°€ êµ¬í•˜ê³ ì í•˜ëŠ” boundaryë¥¼ $y = \mathbf{w}^T\mathbf{x} + b$ ë¼ í•˜ì

ì´ë•Œ, ë²¡í„° $\mathbf{w}$ëŠ” boundaryì™€ ìˆ˜ì§ì´ ëœë‹¤.

**ê°€ì¤‘ì¹˜ ë²¡í„° $\mathbf{w}$ì™€ ì§êµí•˜ë©´ì„œ marginì´ ìµœëŒ€ê°€ ë˜ëŠ” ì„ í˜•ì„ ì°¾ì•„ì•¼ í•œ**

![margin ìœ ë„](../img/Machine-Learning/boundary.png)

ë‹¨ ë‹¤ìŒì€ $y_i(\mathbf{w}^Tx_i + b) \geq 1$ë¥¼ ë§Œì¡±í•´ì•¼ í•œë‹¤.

plus-planeê³¼ minus-plane ê°„ì˜ ê´€ê³„ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.

$x^-$ë¥¼ $\mathbf{w}$ ë°©í–¥ìœ¼ë¡œ $\lambda$ ë¡œ scalingí•˜ì—¬ $\lambda\mathbf{w}$ ì´ë™

$x^+ = x^- + \lambda\mathbf{w}$

$\mathbf{w}^Tx^+ + b = 1$

$\mathbf{w}^T(x^-+\lambda\mathbf{w}) + b = 1$

$\mathbf{w}^Tx^-+\lambda\mathbf{w}^T\mathbf{w} + b = 1$

$\lambda\mathbf{w}^T\mathbf{w} = 2$

$\lambda = \frac{2}{\mathbf{w}^T\mathbf{w}}$

<br>

$margin =\Vert x^+ - x^-\Vert_2$  
$= \Vert x^- + \lambda\mathbf{w}-x^- \Vert_2$  
$= \Vert \lambda\mathbf{w} \Vert_2$  
$= \lambda \sqrt{\mathbf{w}^T\mathbf{w}}$ 
$= \frac{2}{\mathbf{w}^T\mathbf{w}} \sqrt{\mathbf{w}^T\mathbf{w}}$ 
$= \frac{2}{\sqrt{\mathbf{w}^T\mathbf{w}}}$  
$= \frac{2}{\Vert \mathbf{w}\Vert_2}$ 

<br>

### **â‡’ marginì˜ optimization**

margin($=\frac{2}{\Vert \mathbf{w}\Vert_2}$)ì„ ìµœëŒ€í™”í•˜ëŠ” ë¬¸ì œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë³€í˜•í•  ìˆ˜ ìˆìœ¼ë©° 

$max(\frac{2}{\Vert \mathbf{w}\Vert_2})$  â†’ $min(\frac{1}{2}\Vert \mathbf{w}\Vert^2_2)$

ì´ëŠ” **ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•**ì„ í†µí•´ í’€ ìˆ˜ ìˆë‹¤.

$L_p(\mathbf{w}, b, \alpha_i) = \frac{1}{2}\Vert \mathbf{w} \Vert_2^2 - \Sigma_{i=1}^n \alpha_i (y_i(\mathbf{w}^T, \mathbf{x}_i + b) - 1)$

$L_p$ê°€ ìµœëŒ“ê°’ì„ ê°€ì§ˆ  ë•Œ marginì´ ìµœëŒ€ê°€ ëœë‹¤

 

$L_p$ë¥¼ ë¯¸ì§€ìˆ˜ $\mathbf{w}$ì™€ $b$ì— ëŒ€í•´ ê°ê° í¸ë¯¸ë¶„í•œ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

$\frac{\partial L(\mathbf{w}, b, \alpha_i)}{\partial \mathbf{w}} = 0$ 

â†’ $\mathbf{w} = \Sigma_{i=1}^n \alpha_iy_ix_i$      (1)

$\frac{\partial L(\mathbf{w}, b, \alpha_i)}{\partial b} = 0$ 

â†’ $\Sigma_{i=1}^n \alpha_iy_i = 0$           (2)

<br>

**$L_p$ì˜ ì²« ë²ˆì§¸ í•­**

$\frac{1}{2}\Vert \mathbf{w} \Vert_2^2 = \frac{1}{2}\mathbf{w}^T\mathbf{w}$
$= \frac{1}{2}\mathbf{w}^T \Sigma_{j=1}^n\alpha_jy_jx_j$

$= \frac{1}{2} \Sigma_{j=1}^n \alpha_jy_j(\mathbf{w}^Tx_j)$
$= \frac{1}{2} \Sigma_{j=1}^n \alpha_jy_j (\Sigma_{i=1}^n\alpha_iy_ix_i^Tx_j)$

$= \frac{1}{2} \Sigma_{i=1}^n \Sigma_{j=1}^n \alpha_i\alpha_jy_iy_jx_i^Tx_j$


**$L_p$ì˜ ë‘ ë²ˆì§¸ í•­**

$\Sigma_{i=1}^n \alpha_i (y_i(\mathbf{w}^T, \mathbf{x}_i + b) - 1)$ 

$= \Sigma_{i=1}^n \alpha_iy_i(\mathbf{w}^Tx_i + b) - \Sigma_{i=1}^n\alpha_i$

$= \Sigma_{i=1}^n \alpha_iy_i\mathbf{w}^Tx_i + b\Sigma_{i=1}^n \alpha_iy_i - \Sigma_{i=1}^n\alpha_i$

$= \Sigma_{i=1}^n \Sigma_{j=1}^n \alpha_i\alpha_jy_iy_jx_i^Tx_j  - \Sigma_{i=1}^n\alpha_i$

$L_p(\mathbf{w}, b, \alpha_i) = \frac{1}{2}\Vert \mathbf{w} \Vert_2^2 - \Sigma_{i=1}^n \alpha_i (y_i(\mathbf{w}^T, \mathbf{x}_i + b) - 1)$

$= \Sigma_{i=1}^n\alpha_i - \frac{1}{2}\Sigma_{i=1}^n \Sigma_{j=1}^n \alpha_i\alpha_jy_iy_jx_i^Tx_j$ 

<br>

ì°¾ê³ ì í•˜ëŠ” ê±´ marginì´ ìµœëŒ€ê°€ ëœ decision boundary $\mathbf{w}^Tx+b$

$\mathbf{w}$ì™€ $b$ë¥¼ ì°¾ìœ¼ë©´ í•´ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤

 ì‹ (1)ì„ í†µí•´ $\mathbf{w} = \Sigma_{i=1}^n\alpha_iy_ix_i$

ì¦‰ $\alpha$ ê°’ë§Œ ì•Œì•„ë‚´ë©´ $\mathbf{w}$ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤

ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë“¤ì–´ì™”ì„ ë•ŒëŠ” Â $y_i(\mathbf{w}^Tx_i+bâˆ’1)$ì— ë„£ì–´ì„œ

 0ë³´ë‹¤ í¬ë©´ 1, 0ë³´ë‹¤ ì‘ìœ¼ë©´ -1 ë²”ì£¼ë¡œ ì˜ˆì¸¡í•˜ë©´ ëœë‹¤.

<br>

---

### ğŸŒŸ **ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²• ([ë¸”ë¡œê·¸ ë§í¬](https://untitledtblog.tistory.com/96))**  


ì œì•½ ì¡°ê±´Â $g$ë¥¼ ë§Œì¡±í•˜ëŠ”Â $f$ì˜ ìµœì†Ÿê°’ ë˜ëŠ” > ìµœëŒ“ê°’ì€Â $f$ì™€Â $g$ê°€ ì ‘í•˜ëŠ” ì§€ì ì— ì¡´ì¬í•  ìˆ˜ë„ ìˆë‹¤.

![ì œì•½ ì¡°ê±´Â $g(x,y)=c$ë¥¼ ë§Œì¡±í•˜ëŠ”Â $f(x,y)$ì˜ ìµœëŒ“ê°’ì„ êµ¬í•˜ëŠ” ë¬¸ì œ](../img/Machine-Learning/lag.png)

ì œì•½ ì¡°ê±´Â $g(x,y)=c$ë¥¼ ë§Œì¡±í•˜ëŠ”Â $f(x,y)$ì˜ ìµœëŒ“ê°’ì„ êµ¬í•˜ëŠ” ë¬¸ì œ

$f(x,y)$ì˜ ìµœëŒ“ê°’ì„Â $k$ë¼ê³  í•˜ë©´,Â 
$k$ëŠ” $xy$í‰ë©´ì—ì„œ ì§ì„ ì˜ $y$ì¶• ì ˆí¸ì„ ë‚˜íƒ€ë‚¸ë‹¤. 

ì¦‰, ì œì•½ ì¡°ê±´Â $g(x,y)=c$ì™€Â $f(x,y)$ê°€ ì ‘í•  ë•ŒÂ $f(x,y)$ëŠ” ìµœëŒ€ê°€ ëœë‹¤   
(â†”ì œ 3ì‚¬ë¶„ë©´ì—ì„œ ì ‘í•˜ëŠ” ê²½ìš°ëŠ” ìµœì†Œ)

ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•ì—ì„œëŠ” ë‘ í•¨ìˆ˜ê°€ ì ‘í•˜ëŠ”Â ì§€ì ì„ ì°¾ê¸° ìœ„í•´ **ê¸°ìš¸ê¸° ë²¡í„°Â (gradient vector)** ë¥¼ ì´ìš©

$\nabla f=(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y})$

$g$ ì–´ë–¤ ì§€ì ì—ì„œì˜ ì ‘ì„  ë²¡í„°ì™€ ê¸°ìš¸ê¸° ë²¡í„° $\nabla f$ëŠ” ìˆ˜ì§ì„ ì´ë£¬ë‹¤.  
ë”°ë¼ì„œ, ë‘ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸° ë²¡í„°ê°€ ì„œë¡œ ìƒìˆ˜ë°°ì¸ ê´€ê³„ë¥¼ ê°€ì§„ë‹¤.

$\nabla f = \lambda\nabla g$

ì¦‰, $L(x, y, \lambda) = f(x, y) - \lambda(g(x, y) - c)$  
â†’ í•¨ìˆ˜  $L$ì˜ ê¸°ìš¸ê¸° ë²¡í„°ê°€ ì˜ë²¡í„°ê°€ ë˜ëŠ” ì ì„ ì°¾ìœ¼ë©´   
ë‘ í•¨ìˆ˜ $f$ì™€ $g$ê°€ ì ‘í•˜ëŠ” ì§€ì ì„ ì°¾ì„ ìˆ˜ ìˆë‹¤.

---

### â‡’ **SVMì˜ ì¥ì **

- ë²”ì£¼ë‚˜ ìˆ˜ì¹˜ ì˜ˆì¸¡ ë¬¸ì œì— ì‚¬ìš© ê°€ëŠ¥í•¨
- ë°ì´í„° íŠ¹ì„±ì´ ì ì–´ë„ ì„±ëŠ¥ì´ ì¢‹ê²Œ ë‚˜ì˜¤ëŠ” í¸ì´ë©°, ì¡ìŒì— ê°•í•˜ë‹¤.
- ê³¼ì í•©ì„ í”¼í•  ìˆ˜ ìˆë‹¤
- ì €ì°¨ì›ì´ë‚˜ ê³ ì°¨ì›ì˜ ì ì€ ë°ì´í„°ì—ì„œ ì¼ë°˜í™” ëŠ¥ë ¥ì´ ì¢‹ë‹¤.

### â‡’ **SVMì˜ ë‹¨ì **

- ë°ì´í„°ì…‹ì´ ë§ì„ ê²½ìš° í•™ìŠµ ì†ë„ê°€ ëŠë¦¬ë‹¤
- ê³ ì°¨ì›ìœ¼ë¡œ ê°ˆìˆ˜ë¡ ê³„ì‚°ì´ ë¶€ë‹´ì´ ëœë‹¤
- íŒŒë¼ë¯¸í„° ì¡°ì ˆì„ ì˜í•´ì•¼ ìµœì ì˜ ëª¨ë¸ì„ êµ¬í•  ìˆ˜ ìˆë‹¤.
- ì»¤ë„í•¨ìˆ˜ì˜ ì„ íƒì´ ëª…í™•í•˜ì§€ ì•Šë‹¤.

### â‡’ **Outlier**

ë°ì´í„° í¬ì¸í„°ë“¤ì„ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¦¬í•˜ë©´ì„œë„ marginì„ ìµœëŒ€í™”í•´ì•¼í•¨

~ outlierë¥¼ ì–¼ë§Œí¼ í—ˆìš©í•  ê²ƒì¸ì§€ê°€ ì¤‘ìš”í•¨

- **Hard margin**
    
    Outlierë¥¼ í—ˆìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.
    
    ì£¼ì–´ì§„ ëª¨ë“  í•™ìŠµ ë°ì´í„°ë¥¼ ë†“ì¹˜ì§€ ì•Šê³  í•™ìŠµí•˜ë©°, ìƒˆë¡œìš´ data pointì— ëŒ€í•´ì„œ ë¶„ë¥˜ì— ì‹¤íŒ¨í•˜ëŠ” overfitting ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤.
    
    ![Hard margin](../img/Machine-Learning/hard_margin.png)
    
- **Soft margin**
    
    Outlierë“¤ì´ margin ì•ˆì— í¬í•¨ë˜ëŠ” ê²ƒì„ ì–´ëŠ ì •ë„ í—ˆìš©í•˜ë©°,  
    underfitting ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤.
    
    ![Soft margin](../img/Machine-Learning/soft_margin.png)
    
    slack variables $\xi$ë¥¼ ë‘ì–´ ë…¸ì´ì¦ˆ ê°’ì— íŒ¨ë„í‹°ë¥¼ ì£¼ë©° í•™ìŠµ
    
    ì„ í˜•ì ìœ¼ë¡œ ë¶„ë¥˜ë¥¼ í•  ìˆ˜ ì—†ëŠ” ê²½ìš°ì— ë¶„ë¥˜ë¥¼ ìœ„í•´ ì˜¤ì°¨ë¥¼ í—ˆìš©í•´ì•¼ í•˜ëŠ”ë°,   
    ì´ë•Œ ê·œì œ(constraint)ë¥¼ ì™„í™”í•˜ì—¬ **ì˜¤ì°¨ë¥¼ í—ˆìš©í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ë³€ìˆ˜**.
    
    ![slack variables](../img/Machine-Learning/slack.png)
    
    $\xi_i$ì´ 1ë³´ë‹¤ í¬ë©´ ì˜ëª» ë¶„ë¥˜ëœ ë°ì´í„°  
    $\xi_i$ì´ 0ë³´ë‹¤ í¬ê³  1 ì´í•˜ì¼ ë•Œ margin ì•ˆì— ìˆëŠ” ë°ì´í„°  
    $\xi_i$ì´ 0ì¼ ë•Œ ë°ì´í„° í¬ì¸íŠ¸ê°€ support vectorì¼ ê²½ìš°ì´ë‹¤
    
    ![slack variables](../img/Machine-Learning/slack_2.png)
    

**slack variableì´ ë„ì…ëœ decision rule**

$y_i(\mathbf{w}^Tx_i + b) \geq 1 - \xi_i$

### â‡’ **Non-linear SVM**

- **Kernel trick** : ë°ì´í„°ê°€ ì„ í˜•ìœ¼ë¡œ ë¶„ë¦¬ë˜ì§€ ì•ŠëŠ” ê²½ìš°, ì°¨ì›ì„ ë†’ì´ê±°ë‚˜ ë‚®ì¶° ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤.
    - Polynomial
    - RBF (Radial Basis Function) (=Gaussian Kernel)
    
    ![Kernel trick](../img/Machine-Learning/kernel.png)